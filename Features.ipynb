{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01aef19f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graph_tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1b4c43226d8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_tool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 1. Data Simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graph_tool'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import graph_tool.all as gt\n",
    "\n",
    "# 1. Data Simulation\n",
    "\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "num_nodes = 100\n",
    "num_records = 500\n",
    "\n",
    "nodes = [f'company_{i}' for i in range(num_nodes)]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'yearmonth': np.random.choice(['202301', '202302', '202303'], num_records),\n",
    "    'node_origin': np.random.choice(nodes, num_records),\n",
    "    'node_destiny': np.random.choice(nodes, num_records),\n",
    "    'total_value_transactions': np.random.rand(num_records) * 1000,\n",
    "    'quantity_transactions': np.random.randint(1, 5, num_records)\n",
    "})\n",
    "\n",
    "# 2. Graph Construction\n",
    "\n",
    "G = gt.Graph(directed=True)\n",
    "vertex_map = {}\n",
    "weight_prop = G.new_edge_property(\"double\")\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    if row['node_origin'] not in vertex_map:\n",
    "        v1 = G.add_vertex()\n",
    "        vertex_map[row['node_origin']] = v1\n",
    "    else:\n",
    "        v1 = vertex_map[row['node_origin']]\n",
    "\n",
    "    if row['node_destiny'] not in vertex_map:\n",
    "        v2 = G.add_vertex()\n",
    "        vertex_map[row['node_destiny']] = v2\n",
    "    else:\n",
    "        v2 = vertex_map[row['node_destiny']]\n",
    "\n",
    "    e = G.add_edge(v1, v2)\n",
    "    weight_prop[e] = row['total_value_transactions']\n",
    "\n",
    "G.edge_properties[\"weight\"] = weight_prop\n",
    "\n",
    "# 3. Graph Features Calculation\n",
    "\n",
    "in_degree = G.get_in_degrees(G.get_vertices())\n",
    "out_degree = G.get_out_degrees(G.get_vertices())\n",
    "\n",
    "strength_in = G.get_in_degrees(G.get_vertices(), eweight=weight_prop)\n",
    "strength_out = G.get_out_degrees(G.get_vertices(), eweight=weight_prop)\n",
    "\n",
    "eigenvector_centrality, _ = gt.eigenvector(G, weight=weight_prop)\n",
    "clustering_coefficient = gt.local_clustering(G, weight=weight_prop)\n",
    "\n",
    "# Displaying some values for demonstration\n",
    "print(in_degree[:5])\n",
    "print(out_degree[:5])\n",
    "print(strength_in[:5])\n",
    "print(strength_out[:5])\n",
    "print(eigenvector_centrality.a[:5])\n",
    "print(clustering_coefficient.a[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db4153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/giansantoro/ViralDashboard', '/Users/giansantoro/opt/anaconda3/lib/python38.zip', '/Users/giansantoro/opt/anaconda3/lib/python3.8', '/Users/giansantoro/opt/anaconda3/lib/python3.8/lib-dynload', '', '/Users/giansantoro/.local/lib/python3.8/site-packages', '/Users/giansantoro/opt/anaconda3/lib/python3.8/site-packages', '/Users/giansantoro/opt/anaconda3/lib/python3.8/site-packages/aeosa', '/Users/giansantoro/opt/anaconda3/lib/python3.8/site-packages/d3py-0.2.3-py3.8.egg', '/Users/giansantoro/opt/anaconda3/lib/python3.8/site-packages/IPython/extensions', '/Users/giansantoro/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129e10c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/Users/giansantoro/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting snap-stanford\n",
      "  Downloading snap_stanford-6.0.0-cp38-cp38-macosx_10_14_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -andas (/Users/giansantoro/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: snap-stanford\n",
      "Successfully installed snap-stanford-6.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install snap-stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cddbb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   company_id  in_degree  out_degree  strength_in  strength_out  \\\n",
      "0   company_9          5           5  1722.155708   3376.039564   \n",
      "1  company_18          4           7  2987.742820   5233.583908   \n",
      "2  company_57          1           7   663.804528   2523.189640   \n",
      "3  company_95          3           6   346.886492   3819.540565   \n",
      "4   company_0          9           7  5095.014788   2597.197126   \n",
      "\n",
      "   eigenvector_centrality  clustering_coefficient  \n",
      "0                0.345615                0.044444  \n",
      "1                0.165132                0.054545  \n",
      "2                0.069706                0.190476  \n",
      "3                0.047107                0.107143  \n",
      "4                0.500072                0.058333  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6dec1fa14ab1>:35: RuntimeWarning: Weighted directed graph in eigenvector centrality at src/centrality/eigenvector.c:304\n",
      "  eigenvector_centrality = g.eigenvector_centrality(weights='weight', scale=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "\n",
    "# 1. Data Simulation\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "num_nodes = 100\n",
    "num_records = 500\n",
    "\n",
    "nodes = [f'company_{i}' for i in range(num_nodes)]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'yearmonth': np.random.choice(['202301', '202302', '202303'], num_records),\n",
    "    'node_origin': np.random.choice(nodes, num_records),\n",
    "    'node_destiny': np.random.choice(nodes, num_records),\n",
    "    'total_value_transactions': np.random.rand(num_records) * 1000,\n",
    "    'quantity_transactions': np.random.randint(1, 5, num_records)\n",
    "})\n",
    "\n",
    "# 2. Graph Construction\n",
    "unique_nodes = pd.concat([df['node_origin'], df['node_destiny']]).unique()\n",
    "g = ig.Graph(directed=True)\n",
    "g.add_vertices(unique_nodes)\n",
    "g.add_edges(list(zip(df['node_origin'], df['node_destiny'])))\n",
    "g.es['weight'] = df['total_value_transactions'].tolist()\n",
    "\n",
    "# 3. Graph Features Calculation\n",
    "in_degree = g.indegree()\n",
    "out_degree = g.outdegree()\n",
    "\n",
    "strength_in = g.strength(weights='weight', mode='in')\n",
    "strength_out = g.strength(weights='weight', mode='out')\n",
    "\n",
    "eigenvector_centrality = g.eigenvector_centrality(weights='weight', scale=True)\n",
    "clustering_coefficient = g.transitivity_local_undirected(vertices=None, mode=\"zero\")\n",
    "\n",
    "# 4. Constructing the final output dataframe\n",
    "result = pd.DataFrame({\n",
    "    'company_id': unique_nodes,\n",
    "    'in_degree': in_degree,\n",
    "    'out_degree': out_degree,\n",
    "    'strength_in': strength_in,\n",
    "    'strength_out': strength_out,\n",
    "    'eigenvector_centrality': eigenvector_centrality,\n",
    "    'clustering_coefficient': clustering_coefficient\n",
    "})\n",
    "\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55ce550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   company_id  in_degree  out_degree  strength_in  strength_out  \\\n",
      "0   company_9          5           5  1722.155708   3376.039564   \n",
      "1  company_18          4           7  2987.742820   5233.583908   \n",
      "2  company_57          1           7   663.804528   2523.189640   \n",
      "3  company_95          3           6   346.886492   3819.540565   \n",
      "4   company_0          9           7  5095.014788   2597.197126   \n",
      "\n",
      "   eigenvector_centrality  clustering_coefficient  closeness_centrality  \\\n",
      "0                0.345615                0.044444              0.450000   \n",
      "1                0.165132                0.054545              0.445946   \n",
      "2                0.069706                0.190476              0.443946   \n",
      "3                0.047107                0.107143              0.423077   \n",
      "4                0.500072                0.058333              0.495000   \n",
      "\n",
      "   betweenness_centrality  pagerank  ...  in_closeness_centrality  \\\n",
      "0              388.612113  0.015325  ...                 0.357664   \n",
      "1              175.170972  0.006003  ...                 0.310127   \n",
      "2               64.998706  0.004114  ...                 0.263441   \n",
      "3              145.360321  0.006591  ...                 0.320261   \n",
      "4              372.787015  0.013198  ...                 0.384314   \n",
      "\n",
      "   out_closeness_centrality  feedback_centrality  mutual_dyads  \\\n",
      "0                  0.351254                    1            12   \n",
      "1                  0.355072                    0            12   \n",
      "2                  0.360294                    0            12   \n",
      "3                  0.355072                    0            12   \n",
      "4                  0.358974                    1            12   \n",
      "\n",
      "   asymmetric_dyads  null_dyads  personalized_pagerank  \\\n",
      "0               462        4476               0.015325   \n",
      "1               462        4476               0.006003   \n",
      "2               462        4476               0.004114   \n",
      "3               462        4476               0.006591   \n",
      "4               462        4476               0.013198   \n",
      "\n",
      "   assortativity_coefficient  003_triads  102_triads  \n",
      "0                  -0.092165      119556        1950  \n",
      "1                  -0.092165      119556        1950  \n",
      "2                  -0.092165      119556        1950  \n",
      "3                  -0.092165      119556        1950  \n",
      "4                  -0.092165      119556        1950  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-e5ca43c95ef8>:33: RuntimeWarning: Weighted directed graph in eigenvector centrality at src/centrality/eigenvector.c:304\n",
      "  eigenvector_centrality = g.eigenvector_centrality(scale=True, weights='weight')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "\n",
    "# 1. Data Simulation\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "num_nodes = 100\n",
    "num_records = 500\n",
    "\n",
    "nodes = [f'company_{i}' for i in range(num_nodes)]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'yearmonth': np.random.choice(['202301', '202302', '202303'], num_records),\n",
    "    'node_origin': np.random.choice(nodes, num_records),\n",
    "    'node_destiny': np.random.choice(nodes, num_records),\n",
    "    'total_value_transactions': np.random.rand(num_records) * 1000,\n",
    "    'quantity_transactions': np.random.randint(1, 5, num_records)\n",
    "})\n",
    "\n",
    "# 2. Graph Construction\n",
    "unique_nodes = pd.concat([df['node_origin'], df['node_destiny']]).unique()\n",
    "g = ig.Graph(directed=True)\n",
    "g.add_vertices(unique_nodes)\n",
    "g.add_edges(list(zip(df['node_origin'], df['node_destiny'])))\n",
    "g.es['weight'] = df['total_value_transactions'].tolist()\n",
    "\n",
    "# 3. Graph Features Calculation\n",
    "in_degree = g.indegree()\n",
    "out_degree = g.outdegree()\n",
    "strength_in = g.strength(weights='weight', mode='in')\n",
    "strength_out = g.strength(weights='weight', mode='out')\n",
    "eigenvector_centrality = g.eigenvector_centrality(scale=True, weights='weight')\n",
    "clustering_coefficient = g.transitivity_local_undirected(vertices=None, mode=\"zero\")\n",
    "\n",
    "# New Metrics\n",
    "closeness_centrality = g.closeness()\n",
    "betweenness_centrality = g.betweenness()\n",
    "pagerank = g.pagerank()\n",
    "hub_score, authority_score = g.hub_score(), g.authority_score()\n",
    "eccentricity = g.eccentricity()\n",
    "in_closeness_centrality = g.closeness(mode=\"in\")\n",
    "out_closeness_centrality = g.closeness(mode=\"out\")\n",
    "#katz_centrality = g.katz_centrality()\n",
    "feedback_centrality = g.feedback_arc_set()\n",
    "dyad_census = g.dyad_census()\n",
    "triad_census = g.triad_census()\n",
    "personalized_pagerank = g.personalized_pagerank()\n",
    "assortativity_coefficient = g.assortativity_degree(directed=True)\n",
    "#subgraph_centrality = g.subgraph_centrality()\n",
    "feedback_centrality = g.feedback_arc_set()\n",
    "#percolation_centrality = g.community_fastgreedy(weights='weight').as_clustering().membership\n",
    "\n",
    "# 4. Constructing the final output dataframe\n",
    "result = pd.DataFrame({\n",
    "    'company_id': unique_nodes,\n",
    "    'in_degree': in_degree,\n",
    "    'out_degree': out_degree,\n",
    "    'strength_in': strength_in,\n",
    "    'strength_out': strength_out,\n",
    "    'eigenvector_centrality': eigenvector_centrality,\n",
    "    'clustering_coefficient': clustering_coefficient,\n",
    "    'closeness_centrality': closeness_centrality,\n",
    "    'betweenness_centrality': betweenness_centrality,\n",
    "    'pagerank': pagerank,\n",
    "    'hub_score': hub_score,\n",
    "    'authority_score': authority_score,\n",
    "    'eccentricity': eccentricity,\n",
    "    'in_closeness_centrality': in_closeness_centrality,\n",
    "    'out_closeness_centrality': out_closeness_centrality,\n",
    "#    'katz_centrality': katz_centrality,\n",
    "#    'subgraph_centrality': subgraph_centrality,\n",
    "    # 'feedback_centrality': feedback_centrality,  # This returns a list of edges. Needs special handling.\n",
    "#    'percolation_centrality': percolation_centrality,\n",
    "    # 'bridging_centrality': bridging_centrality,  # Derived metric, needs computation\n",
    "    # 'load_centrality': load_centrality  # Derived metric, needs computation\n",
    "})\n",
    "\n",
    "# Adjusting the final dataframe\n",
    "#result['katz_centrality'] = katz_centrality\n",
    "result['feedback_centrality'] = [1 if node in feedback_centrality else 0 for node in range(len(unique_nodes))]  # 1 if node is in feedback set, 0 otherwise\n",
    "result['mutual_dyads'] = dyad_census[0]\n",
    "result['asymmetric_dyads'] = dyad_census[1]\n",
    "result['null_dyads'] = dyad_census[2]\n",
    "result['personalized_pagerank'] = personalized_pagerank\n",
    "result['assortativity_coefficient'] = [assortativity_coefficient] * len(unique_nodes)  # constant for all nodes\n",
    "\n",
    "# Note: Triad Census returns a tuple of 16 values for different configurations.\n",
    "# Extracting some of the critical configurations (you can include others as needed)\n",
    "result['003_triads'] = triad_census[0]  # All three nodes are unconnected\n",
    "result['102_triads'] = triad_census[5]  # Two of the nodes are connected\n",
    "\n",
    "print(result.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "639e8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import dask\n",
    "from dask import delayed, compute\n",
    "\n",
    "# Simulating Data\n",
    "np.random.seed(0)\n",
    "n = 1000\n",
    "node_origin = ['company_' + str(i) for i in np.random.choice(range(n), 10000)]\n",
    "node_destiny = ['company_' + str(i) for i in np.random.choice(range(n), 10000)]\n",
    "total_value_transactions = np.random.rand(10000)\n",
    "quantity_transactions = np.random.randint(1, 100, size=10000)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'node_origin': node_origin,\n",
    "    'node_destiny': node_destiny,\n",
    "    'total_value_transactions': total_value_transactions,\n",
    "    'quantity_transactions': quantity_transactions\n",
    "})\n",
    "\n",
    "unique_nodes = pd.concat([data.node_origin, data.node_destiny]).unique()\n",
    "g = ig.Graph.TupleList(edges=data[['node_origin', 'node_destiny', 'total_value_transactions']].itertuples(index=False), directed=True, weights=True)\n",
    "\n",
    "# Using Dask to Parallelize Computations\n",
    "@dask.delayed\n",
    "def compute_metrics(graph):\n",
    "    # Metrics computation\n",
    "    degree_in = graph.degree(mode='in')\n",
    "    degree_out = graph.degree(mode='out')\n",
    "    strength_in = graph.strength(weights='weight', mode='in')\n",
    "    strength_out = graph.strength(weights='weight', mode='out')\n",
    "    eigenvector_centrality = graph.eigenvector_centrality(weights='weight', scale=True)\n",
    "    clustering_coefficient = graph.transitivity_local_undirected(vertices=None, mode='zero', weights='weight')\n",
    "    betweenness_centrality = graph.betweenness(vertices=None, directed=True, weights='weight')\n",
    "    closeness_centrality = graph.closeness(vertices=None, directed=True, weights='weight')\n",
    "    personalized_pagerank = graph.personalized_pagerank(personalization=None, weights='weight')\n",
    "    assortativity_coefficient = graph.assortativity_degree(directed=True)\n",
    "    #percolation_centrality = graph.community_fastgreedy(weights='weight').as_clustering().membership\n",
    "    \n",
    "    return degree_in, degree_out, strength_in, strength_out, eigenvector_centrality, clustering_coefficient, betweenness_centrality, closeness_centrality, personalized_pagerank, assortativity_coefficient, percolation_centrality\n",
    "\n",
    "# Parallelize computation across cores:\n",
    "results = compute_metrics(g)  \n",
    "computed_results = dask.compute(results)  # Here's the correction.\n",
    "\n",
    "degree_in, degree_out, strength_in, strength_out, eigenvector_centrality, clustering_coefficient, betweenness_centrality, closeness_centrality, personalized_pagerank, assortativity_coefficient, percolation_centrality = computed_results[0]\n",
    "# Constructing the final output dataframe\n",
    "result = pd.DataFrame({\n",
    "    'company_id': unique_nodes,\n",
    "    'degree_in': degree_in,\n",
    "    'degree_out': degree_out,\n",
    "    'strength_in': strength_in,\n",
    "    'strength_out': strength_out,\n",
    "    'eigenvector_centrality': eigenvector_centrality,\n",
    "    'clustering_coefficient': clustering_coefficient,\n",
    "    'betweenness_centrality': betweenness_centrality,\n",
    "    'closeness_centrality': closeness_centrality,\n",
    "    'personalized_pagerank': personalized_pagerank,\n",
    "    'assortativity_coefficient': [assortativity_coefficient] * len(unique_nodes),  # Graph level metric\n",
    "    '#percolation_centrality': percolation_centrality\n",
    "})\n",
    "\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4232d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    company_id  degree_in  degree_out\n",
      "0  company_684         12           7\n",
      "1  company_559         11           9\n",
      "2  company_629          8          13\n",
      "3  company_192         15           8\n",
      "4  company_835          8           8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import dask\n",
    "from dask import delayed, compute\n",
    "\n",
    "# Simulating Data\n",
    "np.random.seed(0)\n",
    "n = 1000\n",
    "node_origin = ['company_' + str(i) for i in np.random.choice(range(n), 10000)]\n",
    "node_destiny = ['company_' + str(i) for i in np.random.choice(range(n), 10000)]\n",
    "total_value_transactions = np.random.rand(10000)\n",
    "quantity_transactions = np.random.randint(1, 100, size=10000)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'node_origin': node_origin,\n",
    "    'node_destiny': node_destiny,\n",
    "    'total_value_transactions': total_value_transactions,\n",
    "    'quantity_transactions': quantity_transactions\n",
    "})\n",
    "\n",
    "unique_nodes = pd.concat([data.node_origin, data.node_destiny]).unique()\n",
    "g = ig.Graph.TupleList(edges=data[['node_origin', 'node_destiny', 'total_value_transactions']].itertuples(index=False), directed=True, weights=True)\n",
    "\n",
    "# Using Dask to Parallelize Computations\n",
    "@dask.delayed\n",
    "def compute_metrics(graph):\n",
    "    # Metrics computation\n",
    "    degree_in = graph.degree(mode='in')\n",
    "    degree_out = graph.degree(mode='out')\n",
    "    #strength_in = graph.strength(weights='weight', mode='in')\n",
    "    #strength_out = graph.strength(weights='weight', mode='out')\n",
    "    #eigenvector_centrality = graph.eigenvector_centrality(weights='weight', scale=True)\n",
    "    #clustering_coefficient = graph.transitivity_local_undirected(vertices=None, mode='zero', weights='weight')\n",
    "    #betweenness_centrality = graph.betweenness(vertices=None, directed=True, weights='weight')\n",
    "    #closeness_centrality = graph.closeness(vertices=None, directed=True, weights='weight')\n",
    "    #personalized_pagerank = graph.personalized_pagerank(personalization=None, weights='weight')\n",
    "    #assortativity_coefficient = graph.assortativity_degree(directed=True)\n",
    "    #percolation_centrality = graph.community_fastgreedy(weights='weight').as_clustering().membership\n",
    "    \n",
    "    #return degree_in, degree_out, strength_in, strength_out, eigenvector_centrality, clustering_coefficient, betweenness_centrality, closeness_centrality, personalized_pagerank, assortativity_coefficient, percolation_centrality\n",
    "    return degree_in, degree_out\n",
    "# Parallelize computation across cores:\n",
    "results = compute_metrics(g)  \n",
    "computed_results = dask.compute(results)  # Here's the correction.\n",
    "\n",
    "#degree_in, degree_out, strength_in, strength_out, eigenvector_centrality, clustering_coefficient, betweenness_centrality, closeness_centrality, personalized_pagerank, assortativity_coefficient, percolation_centrality = computed_results[0]\n",
    "degree_in, degree_out = computed_results[0]\n",
    "# Constructing the final output dataframe\n",
    "result = pd.DataFrame({\n",
    "    'company_id': unique_nodes,\n",
    "    'degree_in': degree_in,\n",
    "    'degree_out': degree_out,\n",
    "    #'strength_in': strength_in,\n",
    "    #'strength_out': strength_out,\n",
    "    #'eigenvector_centrality': eigenvector_centrality,\n",
    "    #'clustering_coefficient': clustering_coefficient,\n",
    "    #'betweenness_centrality': betweenness_centrality,\n",
    "    #'closeness_centrality': closeness_centrality,\n",
    "    #'personalized_pagerank': personalized_pagerank,\n",
    "    #'assortativity_coefficient': [assortativity_coefficient] * len(unique_nodes),  # Graph level metric\n",
    "    #'#percolation_centrality': percolation_centrality\n",
    "})\n",
    "\n",
    "print(result.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
